{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6134f0f4",
   "metadata": {},
   "source": [
    "# Machine Learning Lab – 15 Questions\n",
    "\n",
    "_All solutions in a single notebook. Uses only `numpy`, `pandas`, `matplotlib`, `scikit-learn` (and optional `scipy` for dendrogram)._\n",
    "\n",
    "**Note:** Place the datasets in the **same folder** as this notebook. If a dataset is missing, the cell will **auto-generate** a small synthetic dataset so you can still run and see the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963e578b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Common Imports ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Make plots a bit larger\n",
    "plt.rcParams[\"figure.figsize\"] = (6,4)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import (r2_score, mean_squared_error, mean_absolute_error,\n",
    "                             accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             confusion_matrix, roc_curve, auc, jaccard_score)\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Optional: SciPy for dendrogram (only used if available)\n",
    "try:\n",
    "    from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "    SCIPY_OK = True\n",
    "except Exception:\n",
    "    SCIPY_OK = False\n",
    "\n",
    "import os\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# === Small helpers ===\n",
    "def ensure_csv_or_make(df, fname):\n",
    "    # If fname exists, return pd.read_csv(fname); else save df to fname then read+return.\n",
    "    if os.path.exists(fname):\n",
    "        return pd.read_csv(fname)\n",
    "    else:\n",
    "        df.to_csv(fname, index=False)\n",
    "        return df\n",
    "\n",
    "def plot_actual_vs_pred(y_true, y_pred, title=\"Actual vs Predicted\"):\n",
    "    plt.figure()\n",
    "    plt.scatter(y_true, y_pred, alpha=0.7)\n",
    "    mn = min(y_true.min(), y_pred.min())\n",
    "    mx = max(y_true.max(), y_pred.max())\n",
    "    plt.plot([mn, mx], [mn, mx])\n",
    "    plt.xlabel(\"Actual\")\n",
    "    plt.ylabel(\"Predicted\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def print_regression_metrics(y_true, y_pred):\n",
    "    print(\"R2:\", r2_score(y_true, y_pred))\n",
    "    print(\"MAE:\", mean_absolute_error(y_true, y_pred))\n",
    "    print(\"RMSE:\", mean_squared_error(y_true, y_pred, squared=False))\n",
    "\n",
    "def plot_confusion_matrix_basic(cm, class_names=None, title=\"Confusion Matrix\"):\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest')\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    import numpy as _np\n",
    "    tick_marks = _np.arange(cm.shape[0])\n",
    "    if class_names is None:\n",
    "        class_names = [str(i) for i in range(cm.shape[0])]\n",
    "    plt.xticks(tick_marks, class_names, rotation=45)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                     ha=\"center\", va=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edb4a16",
   "metadata": {},
   "source": [
    "## Q1) Linear Regression – CO₂ Emission & Used Car Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842979b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Task 1A: CO2 Emission prediction ---\n",
    "fname = \"fuel_consumption_dataset.csv\"\n",
    "if not os.path.exists(fname):\n",
    "    np.random.seed(RANDOM_STATE)\n",
    "    n = 200\n",
    "    eng_size = np.random.uniform(1.0, 5.0, size=n)\n",
    "    cyl = np.random.choice([4,6,8], size=n, p=[0.5,0.3,0.2])\n",
    "    fuel_comb = 4 + 2*eng_size + 0.3*cyl + np.random.normal(0, 0.5, size=n)\n",
    "    co2 = 90 + 20*eng_size + 8*cyl + 5*fuel_comb + np.random.normal(0, 10, size=n)\n",
    "    df = pd.DataFrame({\n",
    "        \"ENG_SIZE\": eng_size,\n",
    "        \"CYLINDERS\": cyl,\n",
    "        \"FUELCONSUMPTION_COMB\": fuel_comb,\n",
    "        \"CO2EMISSIONS\": co2\n",
    "    })\n",
    "    df = ensure_csv_or_make(df, fname)\n",
    "\n",
    "data = pd.read_csv(fname)\n",
    "X = data.drop(columns=[data.columns[-1]])\n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "pred = lr.predict(X_test)\n",
    "\n",
    "print(\"=== CO2 Emission – Linear Regression ===\")\n",
    "print_regression_metrics(y_test, pred)\n",
    "plot_actual_vs_pred(y_test.values, pred, title=\"CO2 Emissions: Actual vs Predicted\")\n",
    "\n",
    "# --- Task 1B: Used Car Price prediction ---\n",
    "fname = \"used_cars_dataset.csv\"\n",
    "if not os.path.exists(fname):\n",
    "    np.random.seed(RANDOM_STATE)\n",
    "    n = 300\n",
    "    age = np.random.randint(1, 15, size=n)\n",
    "    mileage = np.random.randint(5000, 200000, size=n)\n",
    "    brand = np.random.choice([\"A\",\"B\",\"C\",\"D\"], size=n)\n",
    "    brand_map = {\"A\":0, \"B\":1, \"C\":2, \"D\":3}\n",
    "    brand_enc = np.vectorize(brand_map.get)(brand)\n",
    "    price = 1200000 - 60000*age - 1.5*mileage + 30000*brand_enc + np.random.normal(0, 50000, size=n)\n",
    "    df2 = pd.DataFrame({\"age\":age, \"mileage\":mileage, \"brand\":brand, \"brand_enc\":brand_enc, \"price\":price})\n",
    "    df2 = ensure_csv_or_make(df2, fname)\n",
    "\n",
    "data2 = pd.read_csv(fname)\n",
    "if \"price\" in data2.columns:\n",
    "    target_col = \"price\"\n",
    "else:\n",
    "    target_col = data2.columns[-1]\n",
    "\n",
    "X2 = data2.select_dtypes(include=[np.number]).drop(columns=[target_col], errors=\"ignore\")\n",
    "y2 = data2[target_col]\n",
    "\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=RANDOM_STATE)\n",
    "lr2 = LinearRegression()\n",
    "lr2.fit(X2_train, y2_train)\n",
    "pred2 = lr2.predict(X2_test)\n",
    "\n",
    "print(\"\\n=== Used Car Price – Linear Regression ===\")\n",
    "print_regression_metrics(y2_test, pred2)\n",
    "plot_actual_vs_pred(y2_test.values, pred2, title=\"Used Car Price: Actual vs Predicted\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b5f6c6",
   "metadata": {},
   "source": [
    "## Q2) L1 (Lasso) & L2 (Ridge) – Housing Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64f37ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fname = \"housing_price_dataset.csv\"\n",
    "if not os.path.exists(fname):\n",
    "    np.random.seed(RANDOM_STATE)\n",
    "    n = 400\n",
    "    rooms = np.random.randint(1, 6, size=n)\n",
    "    area = np.random.randint(300, 3000, size=n)\n",
    "    age = np.random.randint(0, 50, size=n)\n",
    "    price = 50000*rooms + 3000*area - 8000*age + np.random.normal(0, 30000, size=n)\n",
    "    df = pd.DataFrame({\"rooms\":rooms, \"area\":area, \"age\":age, \"price\":price})\n",
    "    df = ensure_csv_or_make(df, fname)\n",
    "\n",
    "data = pd.read_csv(fname)\n",
    "X = data.drop(columns=[data.columns[-1]])\n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE)\n",
    "\n",
    "alphas = np.logspace(-3, 2, 10)\n",
    "\n",
    "best_lasso = None\n",
    "best_r2_lasso = -np.inf\n",
    "for a in alphas:\n",
    "    model = Lasso(alpha=a, random_state=RANDOM_STATE, max_iter=10000)\n",
    "    model.fit(X_train, y_train)\n",
    "    r2 = r2_score(y_test, model.predict(X_test))\n",
    "    if r2 > best_r2_lasso:\n",
    "        best_r2_lasso, best_lasso = r2, model\n",
    "\n",
    "best_ridge = None\n",
    "best_r2_ridge = -np.inf\n",
    "for a in alphas:\n",
    "    model = Ridge(alpha=a, random_state=RANDOM_STATE, max_iter=10000)\n",
    "    model.fit(X_train, y_train)\n",
    "    r2 = r2_score(y_test, model.predict(X_test))\n",
    "    if r2 > best_r2_ridge:\n",
    "        best_r2_ridge, best_ridge = r2, model\n",
    "\n",
    "print(\"=== Lasso Best R2:\", best_r2_lasso, \"alpha:\", best_lasso.alpha)\n",
    "pred_lasso = best_lasso.predict(X_test)\n",
    "plot_actual_vs_pred(y_test.values, pred_lasso, title=\"Housing Price (Lasso): Actual vs Predicted\")\n",
    "\n",
    "print(\"\\n=== Ridge Best R2:\", best_r2_ridge, \"alpha:\", best_ridge.alpha)\n",
    "pred_ridge = best_ridge.predict(X_test)\n",
    "plot_actual_vs_pred(y_test.values, pred_ridge, title=\"Housing Price (Ridge): Actual vs Predicted\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0134776d",
   "metadata": {},
   "source": [
    "## Q3) Linear Regression with One Feature using Gradient Descent (No library) – Salary vs Years Experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dbaedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fname = \"salary_dataset.csv\"\n",
    "if not os.path.exists(fname):\n",
    "    np.random.seed(RANDOM_STATE)\n",
    "    n = 80\n",
    "    Xs = np.random.uniform(0, 15, size=n)\n",
    "    ys = 30000 + 2500*Xs + np.random.normal(0, 3000, size=n)\n",
    "    df = pd.DataFrame({\"YearsExperience\": Xs, \"Salary\": ys})\n",
    "    df = ensure_csv_or_make(df, fname)\n",
    "\n",
    "data = pd.read_csv(fname)\n",
    "x = data.iloc[:, 0].values.reshape(-1,1).astype(float)\n",
    "y = data.iloc[:, 1].values.astype(float)\n",
    "\n",
    "x_mu, x_sigma = x.mean(), x.std()\n",
    "x_norm = (x - x_mu)/x_sigma\n",
    "\n",
    "theta0 = 0.0\n",
    "theta1 = 0.0\n",
    "alpha = 0.1\n",
    "epochs = 50\n",
    "\n",
    "def predict(xn):\n",
    "    return theta0 + theta1 * xn\n",
    "\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    y_hat = predict(x_norm.flatten())\n",
    "    error = y_hat - y\n",
    "    d0 = error.mean()\n",
    "    d1 = (error * x_norm.flatten()).mean()\n",
    "    theta0 -= alpha * d0\n",
    "    theta1 -= alpha * d1\n",
    "\n",
    "    mse = ((y_hat - y)**2).mean()\n",
    "    loss_history.append(mse)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.scatter(x.flatten(), y, s=12)\n",
    "    y_line = (theta0 + theta1 * ((x - x_mu)/x_sigma)).flatten()\n",
    "    plt.plot(x.flatten(), y_line, linewidth=2)\n",
    "    plt.title(f\"Epoch {epoch+1}: Hypothesis vs Data\")\n",
    "    plt.xlabel(\"YearsExperience\")\n",
    "    plt.ylabel(\"Salary\")\n",
    "    plt.show()\n",
    "\n",
    "y_pred_final = (theta0 + theta1 * ((x - x_mu)/x_sigma)).flatten()\n",
    "print(\"Final R2:\", r2_score(y, y_pred_final))\n",
    "print(\"MAE:\", mean_absolute_error(y, y_pred_final))\n",
    "print(\"RMSE:\", mean_squared_error(y, y_pred_final, squared=False))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(loss_history)\n",
    "plt.title(\"MSE Loss per Epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b2a5ab",
   "metadata": {},
   "source": [
    "## Q4) Non-linear Regression – China GDP (Polynomial Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6eec17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fname = \"china_gdp.csv\"\n",
    "if not os.path.exists(fname):\n",
    "    np.random.seed(RANDOM_STATE)\n",
    "    years = np.arange(1960, 2015)\n",
    "    t = (years - 1960)/55.0\n",
    "    gdp = 1e2 + 1e4 / (1 + np.exp(-10*(t-0.6))) + np.random.normal(0, 200, size=len(years))\n",
    "    df = pd.DataFrame({\"Year\": years, \"GDP\": gdp})\n",
    "    df = ensure_csv_or_make(df, fname)\n",
    "\n",
    "data = pd.read_csv(fname)\n",
    "if \"Year\" in data.columns and \"GDP\" in data.columns:\n",
    "    X = data[[\"Year\"]].values\n",
    "    y = data[\"GDP\"].values\n",
    "else:\n",
    "    X = data.iloc[:, [0]].values\n",
    "    y = data.iloc[:, -1].values\n",
    "\n",
    "poly = PolynomialFeatures(degree=4, include_bias=False)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.2, random_state=RANDOM_STATE)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "pred = lr.predict(X_test)\n",
    "\n",
    "print_regression_metrics(y_test, pred)\n",
    "\n",
    "X_full_poly = poly.transform(X)\n",
    "y_full_pred = lr.predict(X_full_poly)\n",
    "plt.figure()\n",
    "plt.scatter(X.flatten(), y, s=12, label=\"Actual\")\n",
    "plt.plot(X.flatten(), y_full_pred, label=\"Model\", linewidth=2)\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"GDP\")\n",
    "plt.title(\"China GDP: Polynomial Regression Fit\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plot_actual_vs_pred(y_test, pred, title=\"China GDP: Actual vs Predicted (Test)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421e4e12",
   "metadata": {},
   "source": [
    "## Q5) Logistic Regression – Cancer (Benign vs Malignant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422313d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fname = \"samples_cancer.csv\"\n",
    "if not os.path.exists(fname):\n",
    "    np.random.seed(RANDOM_STATE)\n",
    "    n = 400\n",
    "    Xsyn = np.random.randn(n, 10)\n",
    "    w = np.array([0.5, -1.2, 0.8, 0.3, -0.6, 1.1, -0.4, 0.9, 0.2, -0.7])\n",
    "    logits = Xsyn @ w + np.random.normal(0, 0.5, size=n)\n",
    "    ysyn = (logits > 0).astype(int)\n",
    "    cols = [f\"f{i}\" for i in range(10)]\n",
    "    df = pd.DataFrame(Xsyn, columns=cols)\n",
    "    df[\"target\"] = ysyn\n",
    "    df = ensure_csv_or_make(df, fname)\n",
    "\n",
    "data = pd.read_csv(fname)\n",
    "ycol = \"target\" if \"target\" in data.columns else data.columns[-1]\n",
    "X = data.drop(columns=[ycol]).values\n",
    "y = data[ycol].values.astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=RANDOM_STATE, stratify=y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)\n",
    "\n",
    "lr = LogisticRegression(max_iter=2000)\n",
    "lr.fit(X_train_s, y_train)\n",
    "pred = lr.predict(X_test_s)\n",
    "proba = lr.predict_proba(X_test_s)[:,1]\n",
    "\n",
    "cm = confusion_matrix(y_test, pred)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, pred))\n",
    "print(\"Precision:\", precision_score(y_test, pred))\n",
    "print(\"Recall:\", recall_score(y_test, pred))\n",
    "print(\"F1-Score:\", f1_score(y_test, pred))\n",
    "plot_confusion_matrix_basic(cm, class_names=[\"Benign\",\"Malignant\"], title=\"Confusion Matrix\")\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, linewidth=2)\n",
    "plt.plot([0,1],[0,1], linestyle=\"--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(f\"ROC Curve (AUC={roc_auc:.3f})\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228246f4",
   "metadata": {},
   "source": [
    "## Q6) KNN – Telecom Customer Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0ed43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fname = \"teleCust.csv\"\n",
    "if not os.path.exists(fname):\n",
    "    np.random.seed(RANDOM_STATE)\n",
    "    n = 500\n",
    "    Xsyn = np.random.randn(n, 5)\n",
    "    ysyn = np.random.choice([0,1,2,3], size=n, p=[0.3,0.3,0.2,0.2])\n",
    "    df = pd.DataFrame(Xsyn, columns=[f\"f{i}\" for i in range(5)])\n",
    "    df[\"category\"] = ysyn\n",
    "    df = ensure_csv_or_make(df, fname)\n",
    "\n",
    "data = pd.read_csv(fname)\n",
    "ycol = \"category\" if \"category\" in data.columns else data.columns[-1]\n",
    "X = data.drop(columns=[ycol]).values\n",
    "y = data[ycol].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=RANDOM_STATE, stratify=y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train_s, y_train)\n",
    "pred = knn.predict(X_test_s)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4e68c9",
   "metadata": {},
   "source": [
    "## Q7) Decision Tree – Drug Prediction (X/Y/C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf90f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fname = \"drug.csv\"\n",
    "if not os.path.exists(fname):\n",
    "    np.random.seed(RANDOM_STATE)\n",
    "    n = 400\n",
    "    Xsyn = np.random.randn(n, 6)\n",
    "    ysyn = np.random.choice([\"drugX\",\"drugY\",\"drugC\"], size=n, p=[0.4,0.4,0.2])\n",
    "    cols = [f\"f{i}\" for i in range(6)]\n",
    "    df = pd.DataFrame(Xsyn, columns=cols)\n",
    "    df[\"drug\"] = ysyn\n",
    "    df = ensure_csv_or_make(df, fname)\n",
    "\n",
    "data = pd.read_csv(fname)\n",
    "ycol = \"drug\" if \"drug\" in data.columns else data.columns[-1]\n",
    "X = data.drop(columns=[ycol]).values\n",
    "y = data[ycol].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=RANDOM_STATE, stratify=y)\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=RANDOM_STATE)\n",
    "dt.fit(X_train, y_train)\n",
    "pred = dt.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, pred))\n",
    "\n",
    "cm = confusion_matrix(y_test, pred, labels=np.unique(y))\n",
    "plot_confusion_matrix_basic(cm, class_names=list(np.unique(y)), title=\"Drug Decision Tree – Confusion Matrix\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463b7f41",
   "metadata": {},
   "source": [
    "## Q8) Naive Bayes – Diabetes (GaussianNB & BernoulliNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27469b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fname = \"pima-indians-diabetes.data.csv\"\n",
    "if not os.path.exists(fname):\n",
    "    np.random.seed(RANDOM_STATE)\n",
    "    n = 768\n",
    "    Xsyn = np.abs(np.random.randn(n, 8))*5\n",
    "    w = np.array([0.3,-0.7,1.1,0.2,-0.5,0.8,-1.0,0.6])\n",
    "    logits = Xsyn @ w + np.random.normal(0, 1, size=n)\n",
    "    ysyn = (logits > np.median(logits)).astype(int)\n",
    "    df = pd.DataFrame(Xsyn, columns=[f\"f{i}\" for i in range(8)])\n",
    "    df[\"Outcome\"] = ysyn\n",
    "    df = ensure_csv_or_make(df, fname)\n",
    "\n",
    "data = pd.read_csv(fname)\n",
    "ycol = \"Outcome\" if \"Outcome\" in data.columns else data.columns[-1]\n",
    "X = data.drop(columns=[ycol]).values\n",
    "y = data[ycol].values.astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=RANDOM_STATE, stratify=y)\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "pred_g = gnb.predict(X_test)\n",
    "print(\"GaussianNB – Accuracy:\", accuracy_score(y_test, pred_g))\n",
    "print(\"GaussianNB – F1:\", f1_score(y_test, pred_g))\n",
    "\n",
    "Xb_train = (X_train > np.median(X_train, axis=0)).astype(int)\n",
    "Xb_test = (X_test > np.median(X_train, axis=0)).astype(int)\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(Xb_train, y_train)\n",
    "pred_b = bnb.predict(Xb_test)\n",
    "print(\"BernoulliNB – Accuracy:\", accuracy_score(y_test, pred_b))\n",
    "print(\"BernoulliNB – F1:\", f1_score(y_test, pred_b))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489d4e0f",
   "metadata": {},
   "source": [
    "## Q9) SVM – Cancer (Linear, Poly, RBF, Sigmoid) – Metrics & ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9151b5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fname = \"samples_cancer.csv\"\n",
    "data = pd.read_csv(fname)\n",
    "ycol = \"target\" if \"target\" in data.columns else data.columns[-1]\n",
    "X = data.drop(columns=[ycol]).values\n",
    "y = data[ycol].values.astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=RANDOM_STATE, stratify=y)\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)\n",
    "\n",
    "kernels = {\n",
    "    \"linear\": {\"kernel\":\"linear\", \"probability\":True},\n",
    "    \"poly\":   {\"kernel\":\"poly\", \"degree\":3, \"probability\":True},\n",
    "    \"rbf\":    {\"kernel\":\"rbf\", \"probability\":True},\n",
    "    \"sigmoid\":{\"kernel\":\"sigmoid\", \"probability\":True}\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "plt.figure()\n",
    "for name, params in kernels.items():\n",
    "    clf = SVC(**params, random_state=RANDOM_STATE)\n",
    "    clf.fit(X_train_s, y_train)\n",
    "    pred = clf.predict(X_test_s)\n",
    "    proba = clf.predict_proba(X_test_s)[:,1]\n",
    "\n",
    "    acc = accuracy_score(y_test, pred)\n",
    "    rec = recall_score(y_test, pred)\n",
    "    prec = precision_score(y_test, pred)\n",
    "    f1 = f1_score(y_test, pred)\n",
    "    jac = jaccard_score(y_test, pred)\n",
    "    err = 1 - acc\n",
    "    cm = confusion_matrix(y_test, pred)\n",
    "\n",
    "    results[name] = {\"acc\":acc, \"rec\":rec, \"prec\":prec, \"f1\":f1, \"jaccard\":jac, \"error\":err, \"cm\":cm}\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_test, proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f\"{name} (AUC={roc_auc:.2f})\")\n",
    "\n",
    "plt.plot([0,1],[0,1], linestyle=\"--\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(\"SVM Kernel Comparison – ROC\")\n",
    "plt.show()\n",
    "\n",
    "for name, d in results.items():\n",
    "    print(f\"\\n=== {name.upper()} ===\")\n",
    "    print(\"Accuracy:\", d[\"acc\"])\n",
    "    print(\"Recall:\", d[\"rec\"])\n",
    "    print(\"Precision:\", d[\"prec\"])\n",
    "    print(\"F1-Score:\", d[\"f1\"])\n",
    "    print(\"Jaccard:\", d[\"jaccard\"])\n",
    "    print(\"Error rate:\", d[\"error\"])\n",
    "    plot_confusion_matrix_basic(d[\"cm\"], class_names=[\"Benign\",\"Malignant\"], title=f\"{name.upper()} – Confusion Matrix\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25abb1f7",
   "metadata": {},
   "source": [
    "## Q10) Diabetes – Compare SVM, Naive Bayes, Decision Tree, KNN (Metrics, Heatmap Confusion, ROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb668b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fname = \"pima-indians-diabetes.data.csv\"\n",
    "data = pd.read_csv(fname)\n",
    "ycol = \"Outcome\" if \"Outcome\" in data.columns else data.columns[-1]\n",
    "X = data.drop(columns=[ycol]).values\n",
    "y = data[ycol].values.astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=RANDOM_STATE, stratify=y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)\n",
    "\n",
    "models = {\n",
    "    \"SVM (RBF)\": SVC(kernel=\"rbf\", probability=True, random_state=RANDOM_STATE),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
    "    \"KNN (k=5)\": KNeighborsClassifier(n_neighbors=5)\n",
    "}\n",
    "\n",
    "probas = {}\n",
    "for name, model in models.items():\n",
    "    if \"SVM\" in name or \"KNN\" in name:\n",
    "        model.fit(X_train_s, y_train)\n",
    "        pred = model.predict(X_test_s)\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            proba = model.predict_proba(X_test_s)[:,1]\n",
    "        else:\n",
    "            proba = model.decision_function(X_test_s)\n",
    "            proba = (proba - proba.min())/(proba.max()-proba.min()+1e-9)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        pred = model.predict(X_test)\n",
    "        proba = model.predict_proba(X_test)[:,1]\n",
    "    probas[name] = (pred, proba)\n",
    "\n",
    "    acc = accuracy_score(y_test, pred)\n",
    "    rec = recall_score(y_test, pred)\n",
    "    prec = precision_score(y_test, pred)\n",
    "    f1 = f1_score(y_test, pred)\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(\"Accuracy:\", acc, \"Recall:\", rec, \"Precision:\", prec, \"F1:\", f1)\n",
    "\n",
    "    cm = confusion_matrix(y_test, pred)\n",
    "    plot_confusion_matrix_basic(cm, class_names=[\"No\",\"Yes\"], title=f\"{name} – Confusion Matrix (Heatmap)\")\n",
    "\n",
    "plt.figure()\n",
    "for name, (_, proba) in probas.items():\n",
    "    fpr, tpr, _ = roc_curve(y_test, proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f\"{name} (AUC={roc_auc:.2f})\")\n",
    "plt.plot([0,1],[0,1], linestyle=\"--\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(\"ROC Comparison – Diabetes\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcffd05",
   "metadata": {},
   "source": [
    "## Q11) Network of Perceptrons – XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c3f5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]], dtype=float)\n",
    "y = np.array([0,1,1,0], dtype=int)\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(2,), activation=\"tanh\",\n",
    "                    solver=\"sgd\", learning_rate_init=0.1, max_iter=2000,\n",
    "                    random_state=RANDOM_STATE)\n",
    "clf.fit(X, y)\n",
    "pred = clf.predict(X)\n",
    "\n",
    "print(\"Predictions on XOR:\", pred)\n",
    "print(\"Accuracy:\", accuracy_score(y, pred))\n",
    "print(\"Hidden weights shape(s):\", [w.shape for w in clf.coefs_])\n",
    "\n",
    "xx, yy = np.meshgrid(np.linspace(-0.5, 1.5, 200), np.linspace(-0.5, 1.5, 200))\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "zz = clf.predict(grid).reshape(xx.shape)\n",
    "\n",
    "plt.figure()\n",
    "plt.contourf(xx, yy, zz, alpha=0.3)\n",
    "plt.scatter(X[:,0], X[:,1], c=y)\n",
    "plt.title(\"XOR learned by 2-2-1 network\")\n",
    "plt.xlabel(\"x1\"); plt.ylabel(\"x2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e260ac3",
   "metadata": {},
   "source": [
    "## Q12) Multilayer Perceptron – Iris (activations & optimizers, metrics, weight/loss/accuracy curves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b1a332",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=RANDOM_STATE, stratify=y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)\n",
    "\n",
    "configs = [\n",
    "    (\"logistic\", \"sgd\"),\n",
    "    (\"tanh\", \"sgd\"),\n",
    "    (\"relu\", \"adam\"),\n",
    "]\n",
    "\n",
    "for act, opt in configs:\n",
    "    clf = MLPClassifier(hidden_layer_sizes=(5,), activation=act, solver=opt,\n",
    "                        learning_rate_init=0.01, max_iter=400, random_state=RANDOM_STATE)\n",
    "    clf.fit(X_train_s, y_train)\n",
    "    pred = clf.predict(X_test_s)\n",
    "    acc = accuracy_score(y_test, pred)\n",
    "    print(f\"\\nActivation={act}, Optimizer={opt}, Test Accuracy={acc:.3f}\")\n",
    "    if hasattr(clf, \"loss_curve_\"):\n",
    "        plt.figure()\n",
    "        plt.plot(clf.loss_curve_)\n",
    "        plt.title(f\"Loss Curve – act={act}, opt={opt}\")\n",
    "        plt.xlabel(\"Iteration\"); plt.ylabel(\"Loss\")\n",
    "        plt.show()\n",
    "\n",
    "    cm = confusion_matrix(y_test, pred)\n",
    "    plot_confusion_matrix_basic(cm, class_names=list(iris.target_names), title=f\"Iris – act={act}, opt={opt} – Confusion\")\n",
    "    print(\"Layer weight shapes:\", [w.shape for w in clf.coefs_])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b1464a",
   "metadata": {},
   "source": [
    "## Q13) K-means Clustering – Customer Segmentation (Euclidean & Manhattan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ab63b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fname = \"Customer segmentation dataset.csv\"\n",
    "if not os.path.exists(fname):\n",
    "    np.random.seed(RANDOM_STATE)\n",
    "    n_per = 125\n",
    "    centers = np.array([[0,0,0,0,0],[5,5,0,0,0],[0,5,5,0,0],[5,0,5,5,0]])\n",
    "    Xsyn = np.vstack([c + np.random.randn(n_per, 5) for c in centers])\n",
    "    df = pd.DataFrame(Xsyn, columns=[f\"f{i}\" for i in range(5)])\n",
    "    df = ensure_csv_or_make(df, fname)\n",
    "\n",
    "data = pd.read_csv(fname)\n",
    "X = data.select_dtypes(include=[np.number]).values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "Xs = scaler.fit_transform(X)\n",
    "\n",
    "k = 4\n",
    "kmeans = KMeans(n_clusters=k, random_state=RANDOM_STATE, n_init=10)\n",
    "labels_euc = kmeans.fit_predict(Xs)\n",
    "\n",
    "p2 = PCA(n_components=2, random_state=RANDOM_STATE).fit_transform(Xs)\n",
    "plt.figure()\n",
    "plt.scatter(p2[:,0], p2[:,1], c=labels_euc, s=10)\n",
    "plt.title(\"KMeans (Euclidean) – 2D PCA\")\n",
    "plt.xlabel(\"PC1\"); plt.ylabel(\"PC2\")\n",
    "plt.show()\n",
    "\n",
    "p3 = PCA(n_components=3, random_state=RANDOM_STATE).fit_transform(Xs)\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(p3[:,0], p3[:,1], p3[:,2], c=labels_euc, s=10)\n",
    "ax.set_title(\"KMeans (Euclidean) – 3D PCA\")\n",
    "plt.show()\n",
    "\n",
    "def manhattan_distance(a, b):\n",
    "    return np.abs(a-b).sum(axis=1)\n",
    "\n",
    "rng = np.random.RandomState(RANDOM_STATE)\n",
    "med_idx = rng.choice(len(Xs), size=k, replace=False)\n",
    "medians = Xs[med_idx].copy()\n",
    "\n",
    "for _ in range(20):\n",
    "    dists = np.stack([manhattan_distance(Xs, m) for m in medians], axis=1)\n",
    "    labels = dists.argmin(axis=1)\n",
    "    new_medians = []\n",
    "    for j in range(k):\n",
    "        cluster_pts = Xs[labels==j]\n",
    "        if len(cluster_pts)==0:\n",
    "            new_medians.append(medians[j])\n",
    "        else:\n",
    "            new_medians.append(np.median(cluster_pts, axis=0))\n",
    "    new_medians = np.array(new_medians)\n",
    "    if np.allclose(new_medians, medians):\n",
    "        break\n",
    "    medians = new_medians\n",
    "\n",
    "labels_manh = labels\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(p2[:,0], p2[:,1], c=labels_manh, s=10)\n",
    "plt.title(\"K-medians (Manhattan) – 2D PCA\")\n",
    "plt.xlabel(\"PC1\"); plt.ylabel(\"PC2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbb8b99",
   "metadata": {},
   "source": [
    "## Q14) Hierarchical Clustering – Agglomerative & Divisive (with Dendrogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18db5957",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fname = \"Vehicle dataset.csv\"\n",
    "if not os.path.exists(fname):\n",
    "    np.random.seed(RANDOM_STATE)\n",
    "    n = 240\n",
    "    Xsyn = np.vstack([np.random.randn(80,6)+i for i in [0,3,6]])\n",
    "    df = pd.DataFrame(Xsyn, columns=[f\"f{i}\" for i in range(6)])\n",
    "    df = ensure_csv_or_make(df, fname)\n",
    "\n",
    "data = pd.read_csv(fname)\n",
    "X = data.select_dtypes(include=[np.number]).values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "Xs = scaler.fit_transform(X)\n",
    "\n",
    "for lk in [\"single\", \"complete\", \"average\"]:\n",
    "    agg = AgglomerativeClustering(n_clusters=3, linkage=lk)\n",
    "    labels = agg.fit_predict(Xs)\n",
    "    p2 = PCA(n_components=2, random_state=RANDOM_STATE).fit_transform(Xs)\n",
    "    plt.figure()\n",
    "    plt.scatter(p2[:,0], p2[:,1], c=labels, s=10)\n",
    "    plt.title(f\"Agglomerative ({lk}) – 2D PCA\")\n",
    "    plt.xlabel(\"PC1\"); plt.ylabel(\"PC2\")\n",
    "    plt.show()\n",
    "\n",
    "if SCIPY_OK:\n",
    "    from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "    Z = linkage(Xs, method=\"average\")\n",
    "    plt.figure()\n",
    "    dendrogram(Z, no_labels=True, count_sort=True)\n",
    "    plt.title(\"Dendrogram (average linkage)\")\n",
    "    plt.xlabel(\"Samples\"); plt.ylabel(\"Distance\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"SciPy not available – skipping dendrogram plot. (Install scipy to enable.)\")\n",
    "\n",
    "def divisive_clustering(Xs, max_depth=3):\n",
    "    clusters = [np.arange(len(Xs))]\n",
    "    for depth in range(max_depth):\n",
    "        sizes = [len(c) for c in clusters]\n",
    "        idx = int(np.argmax(sizes))\n",
    "        big = clusters.pop(idx)\n",
    "        if len(big) < 2:\n",
    "            clusters.insert(idx, big)\n",
    "            break\n",
    "        km = KMeans(n_clusters=2, random_state=RANDOM_STATE, n_init=10)\n",
    "        labels = km.fit_predict(Xs[big])\n",
    "        c0 = big[labels==0]\n",
    "        c1 = big[labels==1]\n",
    "        clusters.extend([c0, c1])\n",
    "    return clusters\n",
    "\n",
    "clusters = divisive_clustering(Xs, max_depth=3)\n",
    "labels_div = np.zeros(len(Xs), dtype=int)\n",
    "for i, c in enumerate(clusters):\n",
    "    labels_div[c] = i\n",
    "\n",
    "p2 = PCA(n_components=2, random_state=RANDOM_STATE).fit_transform(Xs)\n",
    "plt.figure()\n",
    "plt.scatter(p2[:,0], p2[:,1], c=labels_div, s=10)\n",
    "plt.title(\"Divisive (Top-down) approximation – 2D PCA\")\n",
    "plt.xlabel(\"PC1\"); plt.ylabel(\"PC2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed3ba65",
   "metadata": {},
   "source": [
    "## Q15) DBSCAN – Weather Station Clustering (with Outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fa6991",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fname = \"Weather Station dataset.csv\"\n",
    "if not os.path.exists(fname):\n",
    "    np.random.seed(RANDOM_STATE)\n",
    "    n1 = 120; n2 = 100; n3 = 80\n",
    "    c1 = np.random.randn(n1, 2)*0.2 + np.array([0,0])\n",
    "    c2 = np.random.randn(n2, 2)*0.3 + np.array([3,3])\n",
    "    c3 = np.random.randn(n3, 2)*0.25 + np.array([0,4])\n",
    "    noise = np.random.uniform(low=-2, high=6, size=(30,2))\n",
    "    Xsyn = np.vstack([c1,c2,c3,noise])\n",
    "    df = pd.DataFrame(Xsyn, columns=[\"x\",\"y\"])\n",
    "    df = ensure_csv_or_make(df, fname)\n",
    "\n",
    "data = pd.read_csv(fname)\n",
    "X = data.select_dtypes(include=[np.number]).values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "Xs = scaler.fit_transform(X)\n",
    "\n",
    "db = DBSCAN(eps=0.3, min_samples=5)\n",
    "labels = db.fit_predict(Xs)\n",
    "\n",
    "outliers = labels == -1\n",
    "n_out = outliers.sum()\n",
    "print(\"Number of outliers:\", n_out)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X[:,0], X[:,1], c=labels, s=10)\n",
    "plt.title(\"DBSCAN – Clusters (label -1 are outliers)\")\n",
    "plt.xlabel(\"Feature 1\"); plt.ylabel(\"Feature 2\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X[outliers,0], X[outliers,1], s=12)\n",
    "plt.title(\"Outliers detected by DBSCAN\")\n",
    "plt.xlabel(\"Feature 1\"); plt.ylabel(\"Feature 2\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
